main points:
- probabilistic computation is a very convenient way of describing a
  variety of information security issues
  > two primitives: probabilistic interpretation, probabilistic inference
- knowledge tracking summary
  > 
- progression of information flow
  > information flow: does information flow ?
  > quantified information flow: how much information flows ?
  > semantic information flow: what information flows ?

- exploiting models of knowledge
  > auditing
  > policy enforcement
    . simulatable auditing
    . blacklist functions
  > queries on existing knowledge
- problems
  > approximate inference
    . sound approximation
  > initial belief
    . model learner from public statistics
    . which model?

1: title / names

2: convenience of probabilistic programming

Probabilistic programming provides a very convenient way of reasoning
what an agent interacting with system learns about potentially
sensitive information, as long as the system can be described by a
probabilistic program. This is especially convenient if the system is
a program to begin with.

By probabilistic programming I mean the ability to take distributions
over states, evaluate programs over such probabilistic state, retrieve
the distribution over the final states, and perform conditioning on
some aspect of the state (or function output).

3: tracking knowledge

These things let us reason about knowledge an agent attains. As a
silly concrete example let us consider a advertising services,
represented by Bob, wishes to provide Alice some specialized offers
based on her demographic information. You have to suspend your
disbelief a bit here and imagine a world in which people care about
privacy and Alice is somehow able to retain control over her private
information and allows advertisers to evaluate functions over this
information. Justifying this scenario is not relevant to what I would
like to convey.

So let us say Alice has secret age, gender, and predicate engaged?,
while Bob has some belief or distribution over these 3 values, written
B_1~(age, gender, engaged?) or B_1~S where S is the triple. He wants
to determine whether she should be presented a special offer for a
wedding cake:

special-offer?(age, gender, engaged?) :=
  if 21 <= age < 28 and
     gender == 'female and
     engaged? then return true else return false

If the output of this function is true, given Alice's input, and Bob
learns this output, then he would revise his belief about Alice to

B_2~S = B_1~S | (special-offer(B_1~S) = true)

(or "observe special-offer(s) = true; return s")

From B_2~S, the process can be continued to determine what Bob would
know about Alice after additional functions, thereby "tracking
knowledge" Bob would attain from these interactions.

4: outline

- knowledge tracking (done)

- information security

- fine-grained knowledge policies 

So that was probabilistic programming for reasoning about knowledge,
next we will look at how this knowledge-tracking can be used for
information security and the care one must take in implementing
mechanisms for this tasks. This will be brief summary of some of my
existing work. Finally we will talk a bit about how knowledge-tracking
fits in a larger field of information flow and some of my current
thoughts on specifying fine-grained knowledge policies.

5: information security

Looking back to special-offer example, if Alice knew what Bob's
initial belief B_1~S was, she could perform the necessary computation
to track how Bob's belief changes (to B_2~S and so on). This could in
itself be useful as a means of \emph{auditing} knowledge, an offline
task she could do in order to determine the total information revealed
overall.

She could also perform such tracking online, before giving Bob the
function outputs. For example, she could compute B_2~S before
reporting that special-offer(A) = true. Given this potential revised
belief, she could evaluate whether it is too revealing or not, via
some sort of knowledge-based policy P(~S) -> {true,false}. She could
refuse to return the function output if this policy deems the function
unsafe to answer.

6. enforcement care

Alice needs to be careful how she goes about enforcing such
policies. The first issue, which I will mostly glance over, is to make
sure that enforcing the policy does not reveal anything about her
information. This can be accomplished by defining the policy over
beliefs B~S, specifically independent of Alice's actual secret
values. In particular, a policy of the form:

~S(age = A.age) < 0.1

Would not fly. Refusing to answer a query because a policy like that
deemed B_2~S unsafe reveals something about A.age. For some priors, it
might reveal A.age entirely. On the other hand, the policy:

~S(age = a) < 0.1 for every a

Would be ok, as it does not depend on Alice's actual secret value.

The second issue is of computational tractability. Probabilistic
inference is not always an efficient process and depending on the
structure of Bob's prebelief, or the structure of the function to
evaluate, it might be very intractable for Alice to determine B_2~S
exactly.

So Alice can instead use an approximate inference algorithm to
approximate B_2~S to some level of precision. However, she would like
to make sure she is safe regardless of this precision. She can do this
by making sure whatever inference algorithm she uses is sound relative
to the policy she is enforcing. That is, if P(B_2~S) fails, then for
the approximate B'_2~S, P(B'_2~S) also fails.

For our particular example, this would mean that B'_2~S cannot
underapproximate the probability of any age, but can overapproximate
it (and need not be a proper probability distribution). You can see an
example of such a scheme in my recent revision of an older paper,
"Dynamic Enforcement of Knowledge-based Security Policies via Abstract
Interpretation".

7. information flow

information flow/non-interference: Does information flow?
quantified information flow: How much information flows?
"semantic" information flow: What information flows?

Knowledge tracking is a means of reasoning about information flow, a
further generalization of some other techniques for reasoning about
information flow. Back some number of years ago, people were
interested in whether high security data "flows" to low security
observations. In terms of a program, whether high security variable
influences some low security variable. This is the "non-interference"
formulation of the question. Most practically this was done with
either tainting tracking or various type systems with security
labels. 

The yes or no question is too strict as real systems have intended
interference. So eventually arose an interest in not just determining
whether information flows, but how much information flows. This is
formulated using various information theoretic quantities. Entropy,
min-entropy, guessing entropy. Simplifying a bit, the scheme here is
to compare the entropy (or other) of some initial distribution over
secrets to the entropy of the revised distribution (based on some
observed output). So instead of being a yes/no, flow becomes a real
quantity. This quantity can be enforced in either a relative or
absolute sense, preventing larges decreases in entropy in the former,
and preventing the entropy over the posterior distribution from ever
reaching a low level. I say entropy here but there are many proposed
dimensions to this quantity and it unclear which, if any, makes the
most sense.

Ok, tracking knowledge we answer an even more general question: not
whether information flows, not how much, but what information
flows. Let us pretend we can visualize the set of all distributions
over some fixed state-space as this rectangle, though really it is a
much more complex entity. The process of conditioning takes us from
one point to another. In terms of enforcing some limits to knowledge,
we are effectively defining a prohibited zone. The example policy I
defined earlier is an example of a min-entropy threshold:

H_\inft(~S) = max_s ~S(s)

How could Alice justify the use of a knowledge-limit of this form?
Does she really care what is the likelihood of Bob guessing the triple
(age, gender, engaged?) in one try? Doubtful.

On the other hand Alice might have more specific concerns. For
example, she might want to keep her political preference a
secret. This is where having the higher level "semantic" information
view and probabilistic programming can help.

Let us say Alice has determined that the function pol-pref(s) is a
very classifier for political preference given age, gender, engaged?,
she can determine whether Bob's level of knowledge is good at
predicting her political preference (via pol-pref) by computing

B_2~p def= pol-pref(B_2~s)

She might then check that the entropy of this distribution is
sufficiently high as this distribution is much closer to her privacy
concerns than merely B_2~s. If approximate, soundness concerns would
have to be defined relative to this measurement.

8. conclusion

Probabilistic programming is useful for thinking about information
security and implementing protection mechanisms. I read one of the
goals of this workshop is to assess probabilistic programming
languages so I would like to add another criterion to this assessment
specific to use for security; soundness guarantees.
